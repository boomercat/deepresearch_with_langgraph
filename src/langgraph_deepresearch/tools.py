import os
from dotenv import load_dotenv
load_dotenv()
from pathlib import Path
from datetime import datetime
from typing_extensions import Annotated, List, Literal

from langchain.chat_models import init_chat_model 
from langchain_core.messages import HumanMessage
from langchain_core.runnables import RunnableConfig
from langchain_core.tools import tool, InjectedToolArg
from tavily import TavilyClient
from langgraph_deepresearch.utils import get_today_str
from langgraph_deepresearch.state import Summary
from langgraph_deepresearch.prompts import summarize_webpage_prompt






summarization_model =  init_chat_model(
    model=os.getenv("SUMMARY_MODEL_NAME"),          # qwen-max / qwen2.5 / deepseek-r1
    model_provider="openai",                # å…³é”®ï¼šå¼ºåˆ¶èµ° OpenAI-compatible
    api_key=os.getenv("OPENAI_API_KEY"),
    base_url="https://api-inference.modelscope.cn/v1/",
    temperature=0.0,
)

tavily_client = TavilyClient(
    api_key=os.getenv("TAVILY_API_KEY"),
)

#----------æœç´¢å‡½æ•°---------
def tavily_search_multiple(
    search_queries: List[str],
    max_results: int = 3,
    topic: Literal["general", "news", "finance"] = "general",
    include_raw_content: bool = True,
) -> List[dict]:
    """
    ä½¿ç”¨ Tavily API æ‰¹é‡æ‰§è¡Œå¤šä¸ªæœç´¢ queryã€‚

    å‚æ•°:
        search_queries: è¦æ‰§è¡Œçš„æœç´¢æŸ¥è¯¢åˆ—è¡¨ï¼ˆæ¯ä¸ªå…ƒç´ æ˜¯ä¸€æ¡ queryï¼‰
        max_results: æ¯æ¡ query æœ€å¤šè¿”å›å¤šå°‘æ¡ç»“æœ
        topic: æœç´¢çš„ä¸»é¢˜è¿‡æ»¤ï¼ˆå¯é€‰ï¼šgeneral/news/financeï¼‰
        include_raw_content: æ˜¯å¦æŠŠç½‘é¡µåŸå§‹å†…å®¹ä¸€èµ·è¿”å›ï¼ˆç”¨äºåç»­æ‘˜è¦ï¼‰

    è¿”å›:
        ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å¯¹åº”ä¸€æ¡ query çš„ Tavily è¿”å›ç»“æœï¼ˆdictï¼‰
    """

    # è¿™é‡Œæ˜¯â€œä¸²è¡Œâ€æ‰§è¡Œæœç´¢ï¼šä¸€ä¸ª query æœå®Œå†æœä¸‹ä¸€ä¸ª
    # å¤‡æ³¨ï¼šå¦‚æœä½ å¸Œæœ›å¹¶è¡ŒåŠ é€Ÿï¼Œå¯ä»¥ç”¨ AsyncTavilyClient
    search_docs = []
    for query in search_queries:
        result = tavily_client.search(
            query,
            max_results=max_results,
            include_raw_content=include_raw_content,
            topic=topic
        )
        search_docs.append(result)

    return search_docs


def summarize_webpage_content(webpage_content: str) -> str:
    """
    ä½¿ç”¨é…ç½®å¥½çš„ summarization_model å¯¹ç½‘é¡µåŸæ–‡åšæ‘˜è¦ï¼ˆç»“æ„åŒ–è¾“å‡ºï¼‰ã€‚

    å‚æ•°:
        webpage_content: ç½‘é¡µåŸå§‹å†…å®¹ï¼ˆraw contentï¼‰ï¼Œé€šå¸¸æ¯”è¾ƒé•¿

    è¿”å›:
        æ ¼å¼åŒ–åçš„æ‘˜è¦å­—ç¬¦ä¸²ï¼š
        - <summary>...</summary>
        - <key_excerpts>...</key_excerpts>

    å¤±è´¥å…œåº•:
        å¦‚æœæ‘˜è¦å¤±è´¥ï¼Œåˆ™è¿”å›ç½‘é¡µå‰ 1000 å­—ç¬¦ï¼ˆé¿å…ç›´æ¥å´©æ‰ï¼‰
    """
    try:
        # è®© summarization_model ä»¥ç»“æ„åŒ–æ–¹å¼è¾“å‡º Summary schemaï¼ˆé¿å…è‡ªç„¶è¯­è¨€ä¸å¥½è§£æï¼‰
        structured_model = summarization_model.with_structured_output(
            Summary,
            method="function_calling"   # ğŸ‘ˆ å…³é”®ä¿®æ”¹
        )

        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆæ‘˜è¦
        summary = structured_model.invoke([
            HumanMessage(content=summarize_webpage_prompt.format(
                webpage_content=webpage_content,
                date=get_today_str()
            ))
        ])

        # è¾“å‡ºæˆç»Ÿä¸€çš„å¯è¯»æ ¼å¼ï¼ˆæ–¹ä¾¿åç»­æ‹¼æ¥ç»™ research agentï¼‰
        formatted_summary = (
            f"<summary>\n{summary.summary}\n</summary>\n\n"
            f"<key_excerpts>\n{summary.key_excerpts}\n</key_excerpts>"
        )

        return formatted_summary

    except Exception as e:
        # ä»»ä½•å¼‚å¸¸éƒ½å…œåº•ï¼Œé¿å…æœç´¢å·¥å…·æ•´ä½“å¤±è´¥
        print(f"ç½‘é¡µæ‘˜è¦å¤±è´¥: {str(e)}")
        return webpage_content[:1000] + "..." if len(webpage_content) > 1000 else webpage_content

def deduplicate_search_results(search_results: List[dict]) -> dict:
    """
    æ ¹æ® URL å¯¹æœç´¢ç»“æœå»é‡ï¼Œé¿å…åŒä¸€ä¸ªç½‘é¡µè¢«é‡å¤å¤„ç†ï¼ˆæµªè´¹ token + æ—¶é—´ï¼‰ã€‚

    å‚æ•°:
        search_results: tavily_search_multiple è¿”å›çš„ç»“æœåˆ—è¡¨ï¼ˆæ¯ä¸ª query ä¸€ä¸ª dictï¼‰

    è¿”å›:
        dict: {url -> result_dict} çš„æ˜ å°„ï¼Œåªä¿ç•™æ¯ä¸ª URL çš„ç¬¬ä¸€æ¡å‡ºç°ç»“æœ
    """
    unique_results = {}

    for response in search_results:
        for result in response["results"]:
            url = result["url"]
            if url not in unique_results:
                unique_results[url] = result

    return unique_results


def process_search_results(unique_results: dict) -> dict:
    """
    å¯¹å»é‡åçš„ç»“æœåšè¿›ä¸€æ­¥å¤„ç†ï¼šå¦‚æœæœ‰ raw_content å°±åšæ‘˜è¦ï¼Œå¦åˆ™ç”¨çŸ­ content å…œåº•ã€‚

    å‚æ•°:
        unique_results: deduplicate_search_results è¾“å‡ºçš„ {url -> result} å­—å…¸

    è¿”å›:
        summarized_results: {url -> {"title": ..., "content": ...}}ï¼Œ
        å…¶ä¸­ content å·²ç»æ˜¯â€œå¯è¯»æ‘˜è¦â€æˆ–â€œçŸ­å†…å®¹â€
    """
    summarized_results = {}

    for url, result in unique_results.items():
        # å¦‚æœæ²¡æœ‰ raw_contentï¼Œå°±åªèƒ½ç”¨ Tavily è‡ªå¸¦çš„ contentï¼ˆä¸€èˆ¬æ¯”è¾ƒçŸ­ï¼‰
        if not result.get("raw_content"):
            content = result["content"]
        else:
            # æœ‰ raw_content æ—¶ï¼Œä¼˜å…ˆå¯¹åŸæ–‡åšæ‘˜è¦ï¼Œæå‡è´¨é‡å¹¶å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦
            content = summarize_webpage_content(result["raw_content"])

        summarized_results[url] = {
            "title": result["title"],
            "content": content
        }

    return summarized_results



def format_search_output(summarized_results: dict) -> str:
    """
    æŠŠå¤„ç†åçš„æœç´¢ç»“æœæ•´ç†æˆç»Ÿä¸€çš„å­—ç¬¦ä¸²è¾“å‡ºï¼ˆå¸¦ SOURCE åˆ†éš”ï¼‰ã€‚

    å‚æ•°:
        summarized_results: process_search_results çš„è¾“å‡º {url -> {"title","content"}}

    è¿”å›:
        ä¸€ä¸ªæ ¼å¼åŒ–å­—ç¬¦ä¸²ï¼Œå½¢å¦‚ï¼š
        Search results:
        --- SOURCE 1: title ---
        URL: ...
        SUMMARY: ...
        --------------------------------------------------------------------------------
    """
    if not summarized_results:
        return "æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆæœç´¢ç»“æœï¼Œè¯·å°è¯•æ¢ query æˆ–æ›´æ¢æœç´¢ APIã€‚"

    formatted_output = "Search results:\n\n"

    for i, (url, result) in enumerate(summarized_results.items(), 1):
        formatted_output += f"\n\n--- SOURCE {i}: {result['title']} ---\n"
        formatted_output += f"URL: {url}\n\n"
        formatted_output += f"SUMMARY:\n{result['content']}\n\n"
        formatted_output += "-" * 80 + "\n"

    return formatted_output


@tool(parse_docstring=True)
def tavily_search(
    query: str,
    max_results: Annotated[int, InjectedToolArg] = 3,
    topic: Annotated[Literal["general", "news", "finance"], InjectedToolArg] = "general",
) -> str:

    """Fetch results from Tavily search API with content summarization.

    Args:
        query: A single search query to execute
        max_results: Maximum number of results to return
        topic: Topic to filter results by ('general', 'news', 'finance')

    Formatted string of search results with summaries
    """

    # è¿™é‡Œå†…éƒ¨å¤ç”¨ tavily_search_multipleï¼šæŠŠå• query è½¬æˆ listï¼Œç»Ÿä¸€å¤„ç†æµç¨‹
    search_results = tavily_search_multiple(
        [query],
        max_results=max_results,
        topic=topic,
        include_raw_content=True,
    )

    # 1) å…ˆæŒ‰ URL å»é‡
    unique_results = deduplicate_search_results(search_results)

    # 2) å¯¹æ¯ä¸ªç½‘é¡µåšæ‘˜è¦ï¼ˆå¦‚æœæœ‰ raw_contentï¼‰
    summarized_results = process_search_results(unique_results)

    # 3) ç»Ÿä¸€æ ¼å¼åŒ–è¾“å‡º
    return format_search_output(summarized_results)

    # 
    # æ€è€ƒå·¥å…·ï¼šè®© Agent åœ¨æ¯æ¬¡æœç´¢ååšâ€œæˆ˜ç•¥æ€§å¤ç›˜â€ï¼Œé¿å…æ— è„‘ç»§ç»­æœã€‚

    # å»ºè®®ä½¿ç”¨æ—¶æœºï¼š
    # - æ‹¿åˆ°æœç´¢ç»“æœåï¼šæˆ‘æ‰¾åˆ°äº†å“ªäº›å…³é”®äº‹å®ï¼Ÿ
    # - å†³å®šä¸‹ä¸€æ­¥å‰ï¼šæ˜¯å¦å·²è¶³å¤Ÿå›ç­”ï¼Ÿè¿˜æ˜¯éœ€è¦ç»§ç»­æœç´¢ï¼Ÿ
    # - å‘ç°ç¼ºå£æ—¶ï¼šè¿˜ç¼ºå“ªäº›å…³é”®ä¿¡æ¯ï¼Ÿä¸‹ä¸€æ­¥åº”è¯¥æœä»€ä¹ˆï¼Ÿ
    # - ç»“æŸå‰ï¼šè¯æ®æ˜¯å¦å……åˆ†ï¼Ÿèƒ½å¦ç»„ç»‡æˆé«˜è´¨é‡å›ç­”ï¼Ÿ

    # reflection å»ºè®®åŒ…å«å››ç‚¹ï¼š
    # 1) å½“å‰ç»“è®ºï¼šæˆ‘æ‹¿åˆ°äº†å“ªäº›å…·ä½“ä¿¡æ¯ï¼Ÿ
    # 2) ç¼ºå£åˆ†æï¼šè¿˜ç¼ºå“ªäº›å…³é”®ç‚¹ï¼Ÿ
    # 3) è´¨é‡è¯„ä¼°ï¼šè¯æ®/ä¾‹å­æ˜¯å¦è¶³å¤Ÿï¼Ÿ
    # 4) ä¸‹ä¸€æ­¥å†³ç­–ï¼šç»§ç»­æœï¼Ÿè¿˜æ˜¯ç›´æ¥å†™ç­”æ¡ˆï¼Ÿ

    # å‚æ•°:
    #     reflection: å¤ç›˜å†…å®¹ï¼ˆAgent è‡ªå·±å†™çš„ï¼‰

    # è¿”å›:
    #     ç¡®è®¤ä¿¡æ¯ï¼ˆå‘Šè¯‰ Agent å·²è®°å½•å¤ç›˜ï¼‰
    # """
@tool(parse_docstring=True)
def think_tool(reflection: str) -> str:

    """Tool for strategic reflection on research progress and decision-making.
    
    Use this tool after each search to analyze results and plan next steps systematically.
    This creates a deliberate pause in the research workflow for quality decision-making.
    
    When to use:
    - After receiving search results: What key information did I find?
    - Before deciding next steps: Do I have enough to answer comprehensively?
    - When assessing research gaps: What specific information am I still missing?
    - Before concluding research: Can I provide a complete answer now?
    
    Reflection should address:
    1. Analysis of current findings - What concrete information have I gathered?
    2. Gap assessment - What crucial information is still missing?
    3. Quality evaluation - Do I have sufficient evidence/examples for a good answer?
    4. Strategic decision - Should I continue searching or provide my answer?
    
    Args:
        reflection: Your detailed reflection on research progress, findings, gaps, and next steps
        
    Confirmation that reflection was recorded for decision-making
    """
    return f"Reflection recorded: {reflection}"